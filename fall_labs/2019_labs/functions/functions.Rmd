---
title: "Functions"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
tutorial_event_recorder <- function(..., data = NULL) {
  if (!is.null(data)) {
    if(!is.null(data$output)) {
      output <- as.character(data$output)
      if(nchar(output) > 150) {
        output <- paste0(substr(output, 1, 150), "....")
        data$output <- output
      }
    }
  }
  cat("Event - ", paste0(capture.output({dput(list(..., data = data))}), collapse = " "), "\n")
}
options(tutorial.event_recorder = tutorial_event_recorder)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```




## Simulating data with monte carlo simulations

### What is a monte carlo simulation?
Later in Stats I you will be asked to investigate statistical concepts using monte carlo simulations. In a simulation, you repeatedly 

1. generate random samples of data using a known process (e.g. `rnorm()`)
1. make calculations based on the random sample
1. aggregating the results

Functions and loops help us do these repetitious acts efficiently, without repeatedly writing similar code or copying and pasting.

Today, we'll investigate the following property using simulations. 

Statistical theory provides us a way to construct a 95% confidence interval for the expected value of a distribution. In the near future you'll learn about confidence intervals formally. For now, you need to know that if we define a 95% confidence interval, we expect the mean from a random sample to fall within the confidence interval 95 out of 100 times. In other words, we expect the sample mean to be outside of the interval 5 out of 100 times. 

Sometimes we won't be sure if the confidence interval (CI) we defined actually does so. For example, some CI do not work well with small samples; or we may get data that doesn't meet some of the theoretical assumptions. Then we can test this by making 1000s of random samples from the distribution in question and seeing how often the sample mean doesn't fall within the confidence interval. 

If we only made a single random sample, we might be misled. For example, let's draw 30 numbers from a normal distribution with true mean of 0.5 and see if the observed mean appears statistically different from the true mean. 

```{r sim_1, exercise=TRUE, eval=FALSE}
# set seed ensures replicability
set.seed(4)

# we set our parameters
true_mean <- .5
N <- 30

# we simulate and observe outcomes
simulated_data <- rnorm(N, mean = true_mean)
obs_mean <- mean(simulated_data)
obs_mean
```

Wow! The observed mean is twice what we expected! Let's calculate a z-score to put that in perspective. 

Recall a z-score is calculated $\frac{\bar X - \mu}{\frac{s_n}{\sqrt{N}}}$ where $\bar X$ is the observed sample mean, $\mu$ is the true mean, $s_n$ is the observed sample standard deviation and $N$ is the number of observations. 

```{r result_1-setup}
set.seed(4)
true_mean <- .5
N <- 30
simulated_data <- rnorm(N, mean = true_mean)
obs_mean <- mean(simulated_data)
obs_sd <- sd(simulated_data)
zscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N))
```


```{r result_1, exercise=TRUE, exercise.setup="result_1-setup"}
obs_sd <- sd(simulated_data)
zscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N))
zscore
```

We expect the observed mean of this simulated data will be within 1.96 standard deviations of $\mu$ 95 out of 100 times. This observation is 3.3 standard deviations from $\mu$. The probability of that happening by chance is very small: 

particularly we can calculate a p-value. (Read the hint for more information about `pnorm()`).

```{r result_1a-setup}
set.seed(4)
true_mean <- .5
N <- 30
simulated_data <- rnorm(N, mean = true_mean)
obs_mean <- mean(simulated_data)
obs_sd <- sd(simulated_data)
zscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N))
```


```{r pvalue, exercise=TRUE, exercise.setup = "result_1a-setup"}
# plug in the z-score
1 - pnorm()
zscore
```
<div id="pvalue-hint">
  **Hint:** `pnorm()` takes a z-score as the input and returns the probability of observing a value less than or equal to the z-score. So if X is distributed standard normal, `pnorm(z)` $= P(X \leq z)$. (This is the CDF! So `pnorm(.)` is $\Phi(.)$). Why do we subtract $1 -$ `pnorm(.)`?
</div>

That outcome seems surprising, but we could also just have made an unlucky draw. In this workshop, we want to see how often we get such extreme results. We will repeat the steps above 1000 times each, but first we'll write functions that will make this process smooth!


## Writing helper functions to make our monte carlo simulation 

We want to develop functions that 'automate' repeated steps in our monte carlo process. In that way, we can define a few important parameters and run the entire process without rewriting or copying and pasting code over and over again.

As you saw in the motivating example, we must:

1. Simulate data
2. Calculate sample statistics
3. Determine z-scores
4. Test whether the z-score is significant
5. Repeat 1-4 several times 

Finally, we 
6. Measure to what extent our simulations match theory 

If we do this well, we can end up with a single function `our_monte_carlo()` that takes a sample-size `N`, a `true_mean` and number of iterations `B` and returns the proportion of observations that are outside the confidence interval.

Particularly, we are going to write the helper functions "wrapped" by `our_monte_carlo()`. Before proceeding, take a look at the function and guess what the requirements / outputs of each function will be. We'll walk you through designing them, but it helps to think through the process. 

``` 
our_monte_carlo <- function(N, true_mean, B){
  sample_statistics <- monte_carlo_samples(N, true_mean, B)
  z_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)
  test_significance(z_scores, .95) %>% mean()
}
```

## Determine z-scores and check for significance.

We'll start with step 3 and 4 from our simulation outline.

To recap what you saw in the example. Under our assumptions, we can calculate a z-score, which tells us how extreme our observation is. We'll then check if it's outside of our confidence interval. 



![](https://upload.wikimedia.org/wikipedia/commons/2/25/The_Normal_Distribution.svg) 

In our example, the z-score was 3.3, and |3.3| > 1.96. so outside of our 95% confidence interval.



### Determine z-scores

Write a function called `get_zscores` that takes the observed mean and sd, the true mean and N as inputs and returns a z-score as an output.


```{r get_zscores, exercise=TRUE}
get_zscores <- function(obs_mean, true_mean, obs_sd, N){
  
}

# once you write the function run the following code
get_zscores(obs_mean=4.4, true_mean=4.3, obs_sd=.25, N=100) 
```
<div id="get_zscores-hint">
Review the example on page 1.
</div>


```{r quiz-arrange, echo=FALSE}
quiz(caption = "",
  question("What does `get_zscores()` return in the example above",
    answer("4", correct = TRUE),
    answer("40"),
    answer("-4"),
    answer("-40"),
    allow_retry = TRUE
  ))


quiz(caption = "",
  question("The function you wrote should also work on vectorized functions. In the box above, run the following code which takes estimates of the mean and sd from 5 simulations and returns their associated z-scores.

`made_up_means <- c(4.4, 4.1, 4.2, 4.4, 4.2)
made_up_sd <- c(.25, .5, .4, 1, .4)
get_zscores(obs_mean = made_up_means,
            true_mean = 4.3,
            obs_sd = made_up_sd,
            N = 100)`

Let's say we set the critical value at 1.96. Which observation produced is not different from 4.3 in terms of statistically significance? In other words, which observed mean and standard deviation return a |z-score| < 1.96? 

",
    answer("obs_mean=4.4, obs_sd=.25"),
    answer("obs_mean=4.1, obs_sd=.5"),
    answer("obs_mean=4.2, obs_sd=.4"),
    answer("obs_mean=4.4, obs_sd=1", correct = TRUE),
    allow_retry = TRUE
  ))
```


### Check for significance

Once you get a z-score, we want a function to test if the z-score is above or below a cutoff of given level. 


For example, for a two-tailed z-test, the 95% confidence level cutoff is set at `alpha = 1.96`. It's a two-tailed test, so we check if our z-score is below the 2.5th percentile or above the 97.5th percentile. (See the diagram above). 

Write a function `test_significance()` that takes zscores and a given significance level and determines if there is a significant difference at the given level. 

Make sure your code matches the expected output.
```
> test_significance(zscores = 2, significance_level = .95)
[1] TRUE
> test_significance(zscores = c(1.9, -0.3, -3), significance_level = .95)
[1] FALSE FALSE TRUE
```

**note** Recall `qnorm()` will take a probability level and return the cutoff (we often call alpha). e.g. `qnorm(.975)` returns 1.96, the alpha level associated with 95% confidence. Why is .975 used to get the alpha associated with 95% confidence? It's a two-tailed test, so we check if our z-score is from below the 2.5th percentile or above the 97.5th percentile. We will assume we always do a two-tailed test.


```{r test_sig, exercise=TRUE}

```
<div id="test_sig-hint">
If you didn't read the note, read the note! 

Even after reading the note, this might be confusing. Here's additional guidance:

Let's call `significance_level` = $\alpha$. Then, we want the cutoff to be the $(1 - \frac{(1-\alpha)}{2})$-th percentile of the distribution.
</div>


## Simulate data set and find observed mean and sd

We already have `rnorm()` that returns a random sample from a normal distribution with a given mean. So step 1 is built-in to R. However, it will require some finesse to calculate and store the means and standard deviations from the random samples. 


### Building `monte_carlo_samples()`

Recall we want `monte_carlo_samples(N, true_mean, B)` a function that produces `B` random samples from the normal distribution with mean `true_mean` of size `N`. This suggests we'll have `for`-loop. 

In the loop, we want to create random samples and calculate their means. Let's write a function `get_mean_and_sd_from_random_sample()` that does the work inside the for-loop. 

```
> get_mean_and_sd_from_random_sample(N = 30, true_mean = .5)
# A tibble: 1 x 2
   mean    sd
  <dbl> <dbl>
1 0.520 0.878
```

```{r get_mean, exercise=TRUE}





```
**hint** To return two values, place the two values in a tibble or vector.

<div id="get_mean-hint">
Write a function that takes `N`, a sample size, and `true_mean`, the true mean, and returns the observed mean and standard deviation from a random sample from the normal distribution with mean = `true_mean`. (We'll keep standard deviation = 1 which is the `rnorm()` default).
</div>

Finally, write a function that wraps your `get_mean_and_sd_from_random_sample()` in a loop and outputs a tibble with `B` sample means and sample sd where `B` is the number of monte carlo simulations you run.
```
e.g.
> monte_carlo_samples(30, .5, 1000)
# A tibble: 1,000 x 2
mean    sd
<dbl> <dbl>
  1 0.677 1.03
2 0.440 0.902
3 0.377 1.21
4 0.541 0.970
5 0.451 0.990
6 0.528 0.929
7 0.690 1.06
8 0.444 0.959
9 0.573 0.954
10 0.518 0.948
# â€¦ with 990 more rows
```
```{r full-monte-setup}

get_zscores <- function(obs_mean, true_mean, obs_sd, N){
  (obs_mean - true_mean) / (obs_sd / sqrt(N))
}

test_significance <- function(zscores, significance_level) {
  alpha <- qnorm(1 - (1 - significance_level) / 2)
  zscores > alpha | zscores < -alpha
}


get_mean_and_sd_from_random_sample <- function(N, true_mean){
  one_simulation <- rnorm(N, mean = true_mean)
  sim_mean <- mean(one_simulation)
  sim_std_dev <- sd(one_simulation)
  tibble("mean" = sim_mean, "sd" = sim_std_dev)
}


monte_carlo_samples <- function(N, true_mean=.5, B=1000){
  output <- tibble(mean = double(), sd = double())
  for (i in 1:B) {
    output <- output %>% bind_rows(get_mean_and_sd_from_random_sample (N=N, true_mean = true_mean))
  }
  output
}

```

```{r sampler, exercise=TRUE, exercise.setup="full-monte-setup"}

```



## Finally, we simulate

Now you have all the helper functions that are critical for our simulation. We want simulate 1000 sets of 30 data points and then see how often our random sample mean is significantly different from the true mean at 95% confidence. If everything is working as expected, we should see about 5% of the random means appear significantly different.

Final, we write the function `our_monte_carlo()` that does `B` simulations of sample size `N` with a `true_mean`. It returns the fraction of means where we would reject the null hypothesis that the sample mean is statistically indistinguishable from the true mean.

If you really want to test your code, you'll have to copy your functions from the previous pages in to the box below, or, better, open an R script on your computer and put all the code together.

```
> our_monte_carlo(N = 30, true_mean = .5, B = 1000)
[1] 0.052
```

```{r final_monte, exercise=TRUE, exercise.setup="full-monte-setup"}
our_monte_carlo <- function(N, true_mean, B){
  sample_statistics <- monte_carlo_samples(N, true_mean, B)
  z_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)
  test_significance(z_scores, .95) %>% mean()
}
```

<!--add a bit here that makes this all worth it?-->

